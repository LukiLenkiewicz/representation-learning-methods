{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ssl_methods.data_modules import ReconstructionDataModule\n",
    "from ssl_methods.latent_flow.train_module import LatentFlowPretrainingModule\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_module = ReconstructionDataModule(\"../data\", preprocess)\n",
    "data_module.setup()\n",
    "training_module = LatentFlowPretrainingModule()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "# trainer.fit(training_module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Trainer in module pytorch_lightning.trainer.trainer object:\n",
      "\n",
      "class Trainer(builtins.object)\n",
      " |  Trainer(*, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator] = 'auto', strategy: Union[str, pytorch_lightning.strategies.strategy.Strategy] = 'auto', devices: Union[List[int], str, int] = 'auto', num_nodes: int = 1, precision: Union[Literal[64, 32, 16], Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'], Literal['64', '32', '16', 'bf16'], NoneType] = None, logger: Union[pytorch_lightning.loggers.logger.Logger, Iterable[pytorch_lightning.loggers.logger.Logger], bool, NoneType] = None, callbacks: Union[List[pytorch_lightning.callbacks.callback.Callback], pytorch_lightning.callbacks.callback.Callback, NoneType] = None, fast_dev_run: Union[int, bool] = False, max_epochs: Optional[int] = None, min_epochs: Optional[int] = None, max_steps: int = -1, min_steps: Optional[int] = None, max_time: Union[str, datetime.timedelta, Dict[str, int], NoneType] = None, limit_train_batches: Union[int, float, NoneType] = None, limit_val_batches: Union[int, float, NoneType] = None, limit_test_batches: Union[int, float, NoneType] = None, limit_predict_batches: Union[int, float, NoneType] = None, overfit_batches: Union[int, float] = 0.0, val_check_interval: Union[int, float, NoneType] = None, check_val_every_n_epoch: Optional[int] = 1, num_sanity_val_steps: Optional[int] = None, log_every_n_steps: Optional[int] = None, enable_checkpointing: Optional[bool] = None, enable_progress_bar: Optional[bool] = None, enable_model_summary: Optional[bool] = None, accumulate_grad_batches: int = 1, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None, deterministic: Union[bool, Literal['warn'], NoneType] = None, benchmark: Optional[bool] = None, inference_mode: bool = True, use_distributed_sampler: bool = True, profiler: Union[pytorch_lightning.profilers.profiler.Profiler, str, NoneType] = None, detect_anomaly: bool = False, barebones: bool = False, plugins: Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync, List[Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync]], NoneType] = None, sync_batchnorm: bool = False, reload_dataloaders_every_n_epochs: int = 0, default_root_dir: Union[str, pathlib.Path, NoneType] = None) -> None\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator] = 'auto', strategy: Union[str, pytorch_lightning.strategies.strategy.Strategy] = 'auto', devices: Union[List[int], str, int] = 'auto', num_nodes: int = 1, precision: Union[Literal[64, 32, 16], Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'], Literal['64', '32', '16', 'bf16'], NoneType] = None, logger: Union[pytorch_lightning.loggers.logger.Logger, Iterable[pytorch_lightning.loggers.logger.Logger], bool, NoneType] = None, callbacks: Union[List[pytorch_lightning.callbacks.callback.Callback], pytorch_lightning.callbacks.callback.Callback, NoneType] = None, fast_dev_run: Union[int, bool] = False, max_epochs: Optional[int] = None, min_epochs: Optional[int] = None, max_steps: int = -1, min_steps: Optional[int] = None, max_time: Union[str, datetime.timedelta, Dict[str, int], NoneType] = None, limit_train_batches: Union[int, float, NoneType] = None, limit_val_batches: Union[int, float, NoneType] = None, limit_test_batches: Union[int, float, NoneType] = None, limit_predict_batches: Union[int, float, NoneType] = None, overfit_batches: Union[int, float] = 0.0, val_check_interval: Union[int, float, NoneType] = None, check_val_every_n_epoch: Optional[int] = 1, num_sanity_val_steps: Optional[int] = None, log_every_n_steps: Optional[int] = None, enable_checkpointing: Optional[bool] = None, enable_progress_bar: Optional[bool] = None, enable_model_summary: Optional[bool] = None, accumulate_grad_batches: int = 1, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None, deterministic: Union[bool, Literal['warn'], NoneType] = None, benchmark: Optional[bool] = None, inference_mode: bool = True, use_distributed_sampler: bool = True, profiler: Union[pytorch_lightning.profilers.profiler.Profiler, str, NoneType] = None, detect_anomaly: bool = False, barebones: bool = False, plugins: Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync, List[Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync]], NoneType] = None, sync_batchnorm: bool = False, reload_dataloaders_every_n_epochs: int = 0, default_root_dir: Union[str, pathlib.Path, NoneType] = None) -> None\n",
      " |      Customize every aspect of training via flags.\n",
      " |      \n",
      " |      Args:\n",
      " |          accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"mps\", \"auto\")\n",
      " |              as well as custom accelerator instances.\n",
      " |      \n",
      " |          strategy: Supports different training strategies with aliases as well custom strategies.\n",
      " |              Default: ``\"auto\"``.\n",
      " |      \n",
      " |          devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\n",
      " |              (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\n",
      " |              automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\n",
      " |      \n",
      " |          num_nodes: Number of GPU nodes for distributed training.\n",
      " |              Default: ``1``.\n",
      " |      \n",
      " |          precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n",
      " |              16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n",
      " |              Can be used on CPU, GPU, TPUs, HPUs or IPUs.\n",
      " |              Default: ``'32-true'``.\n",
      " |      \n",
      " |          logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
      " |              the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\n",
      " |              ``False`` will disable logging. If multiple loggers are provided, local files\n",
      " |              (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |          callbacks: Add a callback or list of callbacks.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      " |              of train, val and test to find any bugs (ie: a sort of unit test).\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |          max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      " |              If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
      " |              To enable infinite training, set ``max_epochs = -1``.\n",
      " |      \n",
      " |          min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      " |      \n",
      " |          max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
      " |              and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
      " |              ``max_epochs`` to ``-1``.\n",
      " |      \n",
      " |          min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
      " |      \n",
      " |          max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
      " |              The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
      " |              :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
      " |              :class:`datetime.timedelta`.\n",
      " |      \n",
      " |          limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\n",
      " |              Default: ``0.0``.\n",
      " |      \n",
      " |          val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
      " |              after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
      " |              batches. An ``int`` value can only be higher than the number of training batches when\n",
      " |              ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\n",
      " |              across epochs or during iteration-based training.\n",
      " |              Default: ``1.0``.\n",
      " |      \n",
      " |          check_val_every_n_epoch: Perform a validation loop every after every `N` training epochs. If ``None``,\n",
      " |              validation will be done solely based on the number of training batches, requiring ``val_check_interval``\n",
      " |              to be an integer value.\n",
      " |              Default: ``1``.\n",
      " |      \n",
      " |          num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      " |              Set it to `-1` to run all batches in all validation dataloaders.\n",
      " |              Default: ``2``.\n",
      " |      \n",
      " |          log_every_n_steps: How often to log within steps.\n",
      " |              Default: ``50``.\n",
      " |      \n",
      " |          enable_checkpointing: If ``True``, enable checkpointing.\n",
      " |              It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |          enable_progress_bar: Whether to enable to progress bar by default.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |          enable_model_summary: Whether to enable model summarization by default.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |          accumulate_grad_batches: Accumulates gradients over k batches before stepping the optimizer.\n",
      " |              Default: 1.\n",
      " |      \n",
      " |          gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
      " |              gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
      " |              to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
      " |              be set to ``\"norm\"``.\n",
      " |      \n",
      " |          deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
      " |              Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\n",
      " |              that don't support deterministic mode. If not set, defaults to ``False``. Default: ``None``.\n",
      " |      \n",
      " |          benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\n",
      " |              The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\n",
      " |              (``False`` if not manually set). If :paramref:`~pytorch_lightning.trainer.trainer.Trainer.deterministic`\n",
      " |              is set to ``True``, this will default to ``False``. Override to manually set a different value.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\n",
      " |              evaluation (``validate``/``test``/``predict``).\n",
      " |      \n",
      " |          use_distributed_sampler: Whether to wrap the DataLoader's sampler with\n",
      " |              :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for\n",
      " |              strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and\n",
      " |              ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass\n",
      " |              ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed\n",
      " |              sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,\n",
      " |              we don't do this automatically.\n",
      " |      \n",
      " |          profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |          barebones: Whether to run in \"barebones mode\", where all features that may impact raw speed are\n",
      " |              disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training\n",
      " |              runs. The following features are deactivated:\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_checkpointing`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.logger`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_progress_bar`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.log_every_n_steps`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_model_summary`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.num_sanity_val_steps`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.fast_dev_run`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.detect_anomaly`,\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.profiler`,\n",
      " |              :meth:`~pytorch_lightning.core.LightningModule.log`,\n",
      " |              :meth:`~pytorch_lightning.core.LightningModule.log_dict`.\n",
      " |          plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      " |              Default: ``None``.\n",
      " |      \n",
      " |          sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |          reload_dataloaders_every_n_epochs: Set to a positive integer to reload dataloaders every n epochs.\n",
      " |              Default: ``0``.\n",
      " |      \n",
      " |          default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      " |              Default: ``os.getcwd()``.\n",
      " |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``gradient_clip_val`` is not an int or float.\n",
      " |      \n",
      " |          MisconfigurationException:\n",
      " |              If ``gradient_clip_algorithm`` is invalid.\n",
      " |  \n",
      " |  fit(self, model: 'pl.LightningModule', train_dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, val_dataloaders: Optional[Any] = None, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None) -> None\n",
      " |      Runs the full optimization routine.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: Model to fit.\n",
      " |      \n",
      " |          train_dataloaders: An iterable or collection of iterables specifying training samples.\n",
      " |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.train_dataloader` hook.\n",
      " |      \n",
      " |          val_dataloaders: An iterable or collection of iterables specifying validation samples.\n",
      " |      \n",
      " |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.train_dataloader` hook.\n",
      " |      \n",
      " |          ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of two special\n",
      " |              keywords ``\"last\"`` and ``\"hpc\"``. If there is no checkpoint file at the path, an exception is raised.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``model`` is not :class:`~pytorch_lightning.core.LightningModule` for torch version less than\n",
      " |              2.0.0 and if ``model`` is not :class:`~pytorch_lightning.core.LightningModule` or\n",
      " |              :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |  \n",
      " |  init_module(self, empty_init: Optional[bool] = None) -> Generator\n",
      " |      Tensors that you instantiate under this context manager will be created on the device right away and have\n",
      " |      the right data type depending on the precision setting in the Trainer.\n",
      " |      \n",
      " |      The parameters and tensors get created on the device and with the right data type right away without wasting\n",
      " |      memory being allocated unnecessarily. The automatic device placement under this context manager is only\n",
      " |      supported with PyTorch 2.0 and newer.\n",
      " |      \n",
      " |      Args:\n",
      " |          empty_init: Whether to initialize the model with empty weights (uninitialized memory).\n",
      " |              If ``None``, the strategy will decide. Some strategies may not support all options.\n",
      " |              Set this to ``True`` if you are loading a checkpoint into a large model.\n",
      " |  \n",
      " |  predict(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None, return_predictions: Optional[bool] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None) -> Union[List[Any], List[List[Any]], NoneType]\n",
      " |      Run inference on your data. This will call the model forward function to compute predictions. Useful to\n",
      " |      perform distributed and batched predictions. Logging is disabled in the predict hooks.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to predict with.\n",
      " |      \n",
      " |          dataloaders: An iterable or collection of iterables specifying predict samples.\n",
      " |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.predict_dataloader` hook.\n",
      " |      \n",
      " |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.predict_dataloader` hook.\n",
      " |      \n",
      " |          return_predictions: Whether to return predictions.\n",
      " |              ``True`` by default except when an accelerator that spawns processes is used (not supported).\n",
      " |      \n",
      " |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"`` or path to the checkpoint you wish to predict.\n",
      " |              If ``None`` and the model instance was passed, use the current weights.\n",
      " |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |              if a checkpoint callback is configured.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      " |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      " |      \n",
      " |          MisconfigurationException:\n",
      " |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      " |      \n",
      " |          RuntimeError:\n",
      " |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      " |      \n",
      " |      See :ref:`Lightning inference section<deploy/production_basic:Predict step with your LightningModule>` for more.\n",
      " |  \n",
      " |  print(self, *args: Any, **kwargs: Any) -> None\n",
      " |      Print something only on the first process. If running on multiple machines, it will print from the first\n",
      " |      process in each machine.\n",
      " |      \n",
      " |      Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\n",
      " |  \n",
      " |  save_checkpoint(self, filepath: Union[str, pathlib.Path], weights_only: bool = False, storage_options: Optional[Any] = None) -> None\n",
      " |      Runs routine to create a checkpoint.\n",
      " |      \n",
      " |      This method needs to be called on all processes in case the selected strategy is handling distributed\n",
      " |      checkpointing.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: Path where checkpoint is saved.\n",
      " |          weights_only: If ``True``, will only save the model weights.\n",
      " |          storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError:\n",
      " |              If the model is not attached to the Trainer before calling this method.\n",
      " |  \n",
      " |  test(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None, verbose: bool = True, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None) -> List[Mapping[str, float]]\n",
      " |      Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your\n",
      " |      test set until you want to.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to test.\n",
      " |      \n",
      " |          dataloaders: An iterable or collection of iterables specifying test samples.\n",
      " |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.test_dataloader` hook.\n",
      " |      \n",
      " |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"`` or path to the checkpoint you wish to test.\n",
      " |              If ``None`` and the model instance was passed, use the current weights.\n",
      " |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |              if a checkpoint callback is configured.\n",
      " |      \n",
      " |          verbose: If True, prints the test results.\n",
      " |      \n",
      " |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.test_dataloader` hook.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks\n",
      " |          like :meth:`~pytorch_lightning.LightningModule.test_step` etc.\n",
      " |          The length of the list corresponds to the number of test dataloaders used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      " |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      " |      \n",
      " |          MisconfigurationException:\n",
      " |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      " |      \n",
      " |          RuntimeError:\n",
      " |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      " |  \n",
      " |  validate(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None, verbose: bool = True, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None) -> List[Mapping[str, float]]\n",
      " |      Perform one evaluation epoch over the validation set.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to validate.\n",
      " |      \n",
      " |          dataloaders: An iterable or collection of iterables specifying validation samples.\n",
      " |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.val_dataloader` hook.\n",
      " |      \n",
      " |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"`` or path to the checkpoint you wish to validate.\n",
      " |              If ``None`` and the model instance was passed, use the current weights.\n",
      " |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |              if a checkpoint callback is configured.\n",
      " |      \n",
      " |          verbose: If True, prints the validation results.\n",
      " |      \n",
      " |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~pytorch_lightning.core.hooks.DataHooks.val_dataloader` hook.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks\n",
      " |          like :meth:`~pytorch_lightning.LightningModule.validation_step` etc.\n",
      " |          The length of the list corresponds to the number of validation dataloaders used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      " |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      " |      \n",
      " |          MisconfigurationException:\n",
      " |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      " |      \n",
      " |          RuntimeError:\n",
      " |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  accelerator\n",
      " |  \n",
      " |  callback_metrics\n",
      " |      The metrics available to callbacks.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              self.log(\"a_val\", 2.0)\n",
      " |      \n",
      " |      \n",
      " |          callback_metrics = trainer.callback_metrics\n",
      " |          assert callback_metrics[\"a_val\"] == 2.0\n",
      " |  \n",
      " |  checkpoint_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` callback in the\n",
      " |      Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  checkpoint_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` found in\n",
      " |      the Trainer.callbacks list.\n",
      " |  \n",
      " |  current_epoch\n",
      " |      The current epoch, updated after the epoch end hooks are run.\n",
      " |  \n",
      " |  default_root_dir\n",
      " |      The default location to save artifacts of loggers, checkpoints etc.\n",
      " |      \n",
      " |      It is used as a fallback if logger or checkpoint callback do not define specific save paths.\n",
      " |  \n",
      " |  device_ids\n",
      " |      List of device indexes per node.\n",
      " |  \n",
      " |  distributed_sampler_kwargs\n",
      " |  \n",
      " |  early_stopping_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping` callback in the\n",
      " |      Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  early_stopping_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping` found in the\n",
      " |      Trainer.callbacks list.\n",
      " |  \n",
      " |  enable_validation\n",
      " |      Check if we should run validation during training.\n",
      " |  \n",
      " |  estimated_stepping_batches\n",
      " |      The estimated number of batches that will ``optimizer.step()`` during training.\n",
      " |      \n",
      " |      This accounts for gradient accumulation and the current trainer configuration. This might sets up your training\n",
      " |      dataloader if hadn't been set up already.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def configure_optimizers(self):\n",
      " |              optimizer = ...\n",
      " |              stepping_batches = self.trainer.estimated_stepping_batches\n",
      " |              scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\n",
      " |              return [optimizer], [scheduler]\n",
      " |      \n",
      " |      Raises:\n",
      " |          MisconfigurationException:\n",
      " |              If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\n",
      " |              at different epochs.\n",
      " |  \n",
      " |  evaluating\n",
      " |  \n",
      " |  global_rank\n",
      " |  \n",
      " |  global_step\n",
      " |      The number of optimizer steps taken (does not reset each epoch).\n",
      " |      \n",
      " |      This includes multiple optimizers (if enabled).\n",
      " |  \n",
      " |  interrupted\n",
      " |  \n",
      " |  is_global_zero\n",
      " |      Whether this process is the global zero in multi-node training.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              if self.trainer.is_global_zero:\n",
      " |                  print(\"in node 0, accelerator 0\")\n",
      " |  \n",
      " |  is_last_batch\n",
      " |      Whether trainer is executing the last batch.\n",
      " |  \n",
      " |  lightning_module\n",
      " |  \n",
      " |  local_rank\n",
      " |  \n",
      " |  log_dir\n",
      " |      The directory for the current experiment. Use this to save images to, etc...\n",
      " |      \n",
      " |      .. note:: You must call this on all processes. Failing to do so will cause your program to stall forever.\n",
      " |      \n",
      " |       .. code-block:: python\n",
      " |      \n",
      " |           def training_step(self, batch, batch_idx):\n",
      " |               img = ...\n",
      " |               save_img(img, self.trainer.log_dir)\n",
      " |  \n",
      " |  logged_metrics\n",
      " |      The metrics sent to the loggers.\n",
      " |      \n",
      " |      This includes metrics logged via :meth:`~pytorch_lightning.core.LightningModule.log` with the\n",
      " |      :paramref:`~pytorch_lightning.core.LightningModule.log.logger` argument set.\n",
      " |  \n",
      " |  lr_scheduler_configs\n",
      " |  \n",
      " |  max_epochs\n",
      " |  \n",
      " |  max_steps\n",
      " |  \n",
      " |  min_epochs\n",
      " |  \n",
      " |  min_steps\n",
      " |  \n",
      " |  model\n",
      " |      The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\n",
      " |      \n",
      " |      To access the pure LightningModule, use\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.lightning_module` instead.\n",
      " |  \n",
      " |  node_rank\n",
      " |  \n",
      " |  num_devices\n",
      " |      Number of devices the trainer uses per node.\n",
      " |  \n",
      " |  num_nodes\n",
      " |  \n",
      " |  num_predict_batches\n",
      " |      The number of prediction batches that will be used during ``trainer.predict()``.\n",
      " |  \n",
      " |  num_sanity_val_batches\n",
      " |      The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.\n",
      " |  \n",
      " |  num_test_batches\n",
      " |      The number of test batches that will be used during ``trainer.test()``.\n",
      " |  \n",
      " |  num_training_batches\n",
      " |      The number of training batches that will be used during ``trainer.fit()``.\n",
      " |  \n",
      " |  num_val_batches\n",
      " |      The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.\n",
      " |  \n",
      " |  precision\n",
      " |  \n",
      " |  precision_plugin\n",
      " |  \n",
      " |  predict_dataloaders\n",
      " |      The prediction dataloader(s) used during ``trainer.predict()``.\n",
      " |  \n",
      " |  progress_bar_callback\n",
      " |      An instance of :class:`~pytorch_lightning.callbacks.progress.progress_bar.ProgressBar` found in the\n",
      " |      Trainer.callbacks list, or ``None`` if one doesn't exist.\n",
      " |  \n",
      " |  progress_bar_metrics\n",
      " |      The metrics sent to the progress bar.\n",
      " |      \n",
      " |      This includes metrics logged via :meth:`~pytorch_lightning.core.LightningModule.log` with the\n",
      " |      :paramref:`~pytorch_lightning.core.LightningModule.log.prog_bar` argument set.\n",
      " |  \n",
      " |  received_sigterm\n",
      " |      Whether a ``signal.SIGTERM`` signal was received.\n",
      " |      \n",
      " |      For example, this can be checked to exit gracefully.\n",
      " |  \n",
      " |  scaler\n",
      " |  \n",
      " |  strategy\n",
      " |  \n",
      " |  test_dataloaders\n",
      " |      The test dataloader(s) used during ``trainer.test()``.\n",
      " |  \n",
      " |  train_dataloader\n",
      " |      The training dataloader(s) used during ``trainer.fit()``.\n",
      " |  \n",
      " |  val_dataloaders\n",
      " |      The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.\n",
      " |  \n",
      " |  world_size\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ckpt_path\n",
      " |      Set to the path/URL of a checkpoint loaded via :meth:`~pytorch_lightning.trainer.trainer.Trainer.fit`,\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.validate`,\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.test`, or\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.predict`.\n",
      " |      \n",
      " |      ``None`` otherwise.\n",
      " |  \n",
      " |  logger\n",
      " |      The first :class:`~pytorch_lightning.loggers.logger.Logger` being used.\n",
      " |  \n",
      " |  loggers\n",
      " |      The list of :class:`~pytorch_lightning.loggers.logger.Logger` used.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          for logger in trainer.loggers:\n",
      " |              logger.log_metrics({\"foo\": 1.0})\n",
      " |  \n",
      " |  optimizers\n",
      " |  \n",
      " |  predicting\n",
      " |  \n",
      " |  sanity_checking\n",
      " |      Whether sanity checking is running.\n",
      " |      \n",
      " |      Useful to disable some hooks, logging or callbacks during the sanity checking.\n",
      " |  \n",
      " |  testing\n",
      " |  \n",
      " |  training\n",
      " |  \n",
      " |  validating\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ModelCheckpoint' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ModelCheckpoint' object is not callable"
     ]
    }
   ],
   "source": [
    "trainer.checkpoint_callback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
