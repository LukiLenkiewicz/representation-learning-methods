{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hC8VhuEoR90S"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import STL10\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torchmetrics import F1Score\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess =  transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ../data\\stl10_binary.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2640397119/2640397119 [04:10<00:00, 10533471.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data\\stl10_binary.tar.gz to ../data\n"
          ]
        }
      ],
      "source": [
        "train_dataset = STL10(\"../data\", split=\"train\", download=True, transform=preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "test_dataset = STL10(\"../data\", split=\"test\", download=True, transform=preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "unlabeled_dataset = STL10(\"../data\", split=\"unlabeled\", download=True, transform=preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
        "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PretrainingModule(pl.LightningModule):\n",
        "    def __init__(self, model, learning_rate, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.accuracy = F1Score(task=\"multiclass\", average=\"macro\", num_classes=num_classes)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, text):\n",
        "        return self.model(text)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        img, label = batch\n",
        "        output = self(img)\n",
        "        loss = self.loss_fn(output, img)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        img, label = batch\n",
        "        output = self(img)\n",
        "        loss = self.loss_fn(output, img)\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        img, label = batch\n",
        "        output = self(img)\n",
        "        loss = self.loss_fn(output, img)\n",
        "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200], gamma=0.1)\n",
        "        # return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainingModule(pl.LightningModule):\n",
        "    def __init__(self, model, learning_rate, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.accuracy = F1Score(task=\"multiclass\", average=\"macro\", num_classes=num_classes)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, text):\n",
        "        return self.model(text)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        img, label = batch\n",
        "        output = self(img)\n",
        "        loss = self.loss_fn(output, label)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        img, label = batch\n",
        "        output = self(img)\n",
        "        loss = self.loss_fn(output, label)\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        img, label = batch\n",
        "        output = self(img)\n",
        "        loss = self.loss_fn(output, label)\n",
        "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200], gamma=0.1)\n",
        "        # return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class STLDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((32, 32)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.stl10_train = STL10(root='../data', split=\"train\", download=True, transform=self.transform)\n",
        "        self.stl10_test = STL10(root='../data', split=\"test\", download=True, transform=self.transform)\n",
        "        self.stl10_unlabeled = STL10(root='../data', split=\"unlabeled\", download=True, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.stl10_train, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.stl10_test, batch_size=self.batch_size, shuffle=False)\n",
        "    \n",
        "    def unlabeled_dataloader(self):\n",
        "        return DataLoader(self.stl10_unlabeled, batch_size=self.batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __main__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * BasicBlock.expansion, 1000)\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * BasicBlock.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * BasicBlock.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# resnet = ResNet18()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import os\n",
        "\n",
        "class ResNetEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetEncoder, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.fc = nn.Linear(resnet.fc.in_features, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = self.resnet(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Linear(100, 256 * 4 * 4)\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = F.relu(self.fc(z))\n",
        "        z = z.view(z.size(0), 256, 4, 4)\n",
        "        z = F.relu(self.deconv1(z))\n",
        "        z = F.relu(self.deconv2(z))\n",
        "        z = torch.tanh(self.deconv3(z))\n",
        "        return z\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512 + 100, 1)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = torch.cat([x, z], dim=1)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "class BiGAN(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(BiGAN, self).__init__()\n",
        "        self.encoder = ResNetEncoder()\n",
        "        self.generator = Generator()\n",
        "        self.discriminator = Discriminator()\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.automatic_optimization = False\n",
        "    \n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        lr = 0.0002\n",
        "        beta1 = 0.5\n",
        "        optimizerE = optim.Adam(self.encoder.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "        optimizerG = optim.Adam(self.generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "        optimizerD = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "        return [optimizerE, optimizerG, optimizerD]\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, _ = batch\n",
        "        batch_size = images.size(0)\n",
        "        real_labels = torch.ones(batch_size, 1).type_as(images)\n",
        "        fake_labels = torch.zeros(batch_size, 1).type_as(images)\n",
        "        \n",
        "        optE, optG, optD = self.optimizers()\n",
        "\n",
        "        z = torch.randn(batch_size, 100).type_as(images)\n",
        "        fake_images = self.generator(z)\n",
        "        real_outputs = self.discriminator(images, self.encoder(images))\n",
        "        fake_outputs = self.discriminator(fake_images, z)\n",
        "        d_loss_real = self.criterion(real_outputs, real_labels)\n",
        "        d_loss_fake = self.criterion(fake_outputs, fake_labels)\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        \n",
        "        optD.zero_grad()\n",
        "        self.manual_backward(d_loss, retain_graph=True)\n",
        "        optD.step()\n",
        "\n",
        "        fake_outputs = self.discriminator(fake_images, z)\n",
        "        g_loss = self.criterion(fake_outputs, real_labels)\n",
        "        \n",
        "        optG.zero_grad()\n",
        "        optE.zero_grad()\n",
        "        self.manual_backward(g_loss)\n",
        "        optG.step()\n",
        "        optE.step()\n",
        "        \n",
        "        self.log('d_loss', d_loss, prog_bar=True)\n",
        "        self.log('g_loss', g_loss, prog_bar=True)\n",
        "        \n",
        "        return {'d_loss': d_loss, 'g_loss': g_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "stl10_dm = STLDataModule(batch_size=32)\n",
        "stl10_dm.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Adam\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Adam\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "c:\\Users\\Adam\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "c:\\Users\\Adam\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Adam\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:652: Checkpoint directory C:\\Users\\Adam\\Desktop\\PGM_UR\\representation-learning-methods\\notebooks\\checkpoints exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type          | Params | Mode \n",
            "--------------------------------------------------------\n",
            "0 | encoder       | ResNetEncoder | 11.2 M | train\n",
            "1 | generator     | Generator     | 1.1 M  | train\n",
            "2 | discriminator | Discriminator | 2.8 M  | train\n",
            "3 | criterion     | BCELoss       | 0      | train\n",
            "--------------------------------------------------------\n",
            "15.1 M    Trainable params\n",
            "0         Non-trainable params\n",
            "15.1 M    Total params\n",
            "60.229    Total estimated model params size (MB)\n",
            "c:\\Users\\Adam\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/157 [00:00<?, ?it/s] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Adam\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99: 100%|██████████| 157/157 [00:04<00:00, 34.77it/s, v_num=47, d_loss=0.0768, g_loss=10.30] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99: 100%|██████████| 157/157 [00:04<00:00, 34.75it/s, v_num=47, d_loss=0.0768, g_loss=10.30]\n"
          ]
        }
      ],
      "source": [
        "bigan_model = BiGAN()\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='d_loss',\n",
        "    dirpath='./checkpoints',\n",
        "    filename='bigan-{epoch:02d}-{d_loss:.2f}',\n",
        "    save_top_k=3,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=100,\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n",
        "trainer.fit(bigan_model, stl10_dm)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
